{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('IMDB-dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    0.5\n",
       "0    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'] = dataset['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Ratio of positive and negative reviews\n",
    "dataset['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = dataset['review'][:40000]\n",
    "y_train = dataset['sentiment'][:40000]\n",
    "\n",
    "X_test_texts = dataset['review'][40000:41000]\n",
    "y_test = dataset['sentiment'][40000:41000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 400001\n",
    "embedding_dim = 100\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_to_vec_map = {}\n",
    "with open('embeddings/glove.6B.100d.txt', 'r', encoding='utf-8') as f:    \n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        \n",
    "        word_to_index[word] = i\n",
    "        index_to_word[i] = word\n",
    "        word_to_vec_map[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'providing', (100,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['the'], index_to_word[2066], word_to_vec_map['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Fill in the embedding matrix\n",
    "for word, index in word_to_index.items():\n",
    "    if word in word_to_vec_map:\n",
    "        embedding_matrix[index] = word_to_vec_map[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 2500\n",
    "\n",
    "def tokenize(sentences, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    tokenized_sentences = np.zeros((len(sentences), max_sentence_length))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = sentence.lower().split()\n",
    "        \n",
    "        for j, word in enumerate(words):    \n",
    "            # Remove punctuation from the word\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "                        \n",
    "            if word in word_to_index:\n",
    "                tokenized_sentences[i, j] = word_to_index[word]\n",
    "            else:\n",
    "                tokenized_sentences[i, j] = 400000 # Vector for unknown words\n",
    "        \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenize(X_train_texts)\n",
    "X_test_sequences = tokenize(X_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "This model is a baseline for the other models. It is a simple neural network that consists of an embedding layer, a global average pooling layer, and a dense layer with sigmoid activation. **The embedding layer is trained from scratch.** Where the other models will use pre-trained embeddings, because training time would be too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences, y_train, epochs=20, batch_size=16, validation_data=(X_test_sequences, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "- Small training set (1000 examples)\n",
    "- Training unitl reaches max accuracy (20 to 60 epochs)\n",
    "- Batch size of 16\n",
    "- Adam optimizer\n",
    "\n",
    "| Model | Accuracy | Validation Accuracy |\n",
    "|-------|----------|---------------------|\n",
    "| Baseline | 1.0000 | 0.8260 |\n",
    "| LSTM 32 | 0.4970 | 0.5320 |\n",
    "| Bi-LSTM 8 | 0.9300 | 0.7460 |\n",
    "| 2 x Bi-LSTM 8 | 0.9350 | 0.7360 |\n",
    "| Bi-LSTM 16 | 0.9840 | 0.7860 |\n",
    "| Bi-LSTM 32 | 0.9990  | 0.7840 |\n",
    "| Bi-LSTM 128 | 0.9930 | 0.7320 |\n",
    "| 2 x Bi-LSTM 128 | 0.9990 | 0.7320 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Simple baseline model performs surprisingly well although it's overfitting\n",
    "- LSTM layers with > 32 units introduce overfitting\n",
    "- LSTM layers with < 16 units can't learn complex function\n",
    "- Bidirectional LSTM layers give much better results\n",
    "- Adding second layer doesn't improve performance, only increases training time\n",
    "- Baseline model quickly reaches its best performance - it can't learn more complex function\n",
    "\n",
    "#### Conclusions\n",
    "- Use single bidirectional LSTM layers with 16-32 units\n",
    "- Use dropout layers to prevent overfitting\n",
    "- Train on entire dataset to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_SENTENCE_LENGTH, weights=[embedding_matrix], trainable=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 788s 315ms/step - loss: 0.4380 - accuracy: 0.8009 - val_loss: 0.3764 - val_accuracy: 0.8370\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 804s 322ms/step - loss: 0.3424 - accuracy: 0.8524 - val_loss: 0.3587 - val_accuracy: 0.8640\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 745s 298ms/step - loss: 0.3137 - accuracy: 0.8651 - val_loss: 0.3441 - val_accuracy: 0.8510\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 729s 292ms/step - loss: 0.2884 - accuracy: 0.8785 - val_loss: 0.3094 - val_accuracy: 0.8720\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 776s 310ms/step - loss: 0.2695 - accuracy: 0.8878 - val_loss: 0.3032 - val_accuracy: 0.8830\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 725s 290ms/step - loss: 0.2537 - accuracy: 0.8942 - val_loss: 0.2944 - val_accuracy: 0.8830\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 699s 279ms/step - loss: 0.2380 - accuracy: 0.9020 - val_loss: 0.2957 - val_accuracy: 0.8820\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 691s 276ms/step - loss: 0.2254 - accuracy: 0.9083 - val_loss: 0.2947 - val_accuracy: 0.8780\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 849s 340ms/step - loss: 0.2128 - accuracy: 0.9140 - val_loss: 0.2915 - val_accuracy: 0.8790\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 708s 283ms/step - loss: 0.2014 - accuracy: 0.9182 - val_loss: 0.2867 - val_accuracy: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2322d79f090>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sequences, y_train, epochs=10, batch_size=16, validation_data=(X_test_sequences, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- Training on entire dataset significantly reduced overfitting\n",
    "- Model with single Bidirectional LSTM layer got **91.8%** accuracy on training set and **88.5%** accuracy on a test set. There was no validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = [\n",
    "    \"The movie was great. I enjoyed every second of it although actors play wasn't perfect but plot made up for it. I would recommend it to others.\",\n",
    "    \"I loved this movie. It was so funny and entertaining.\",\n",
    "    \"This movie was terrible. The plot was boring and the acting was awful.\",\n",
    "    \"Funny movie but not an amazing one. Best to watch with your friends.\",\n",
    "    \"Not a good movie. I wasn't satisfied with the actors play. I wouldn't recommend it to others.\",\n",
    "    \"A good movie. I was satisfied with the actors play. I would recommend it to others.\"\n",
    "]\n",
    "\n",
    "review_sequence = tokenize(review)\n",
    "\n",
    "prediction = model.predict(review_sequence).flatten() > 0.5\n",
    "\n",
    "# Map True to Positive and False to Negative\n",
    "np.where(prediction, \"Positive\", \"Negative\").tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
