{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/IMDB-dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43483"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset['review'].str.split().str.len() < 400).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: sentiment, dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'] = dataset['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Ratio of positive and negative reviews\n",
    "dataset['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = dataset['review'][:49000]\n",
    "y_train = dataset['sentiment'][:49000]\n",
    "\n",
    "X_test_texts = dataset['review'][49000:]\n",
    "y_test = dataset['sentiment'][49000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 400001\n",
    "embedding_dim = 100\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_to_vec_map = {}\n",
    "with open('embeddings/glove.6B.100d.txt', 'r', encoding='utf-8') as f:    \n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        \n",
    "        word_to_index[word] = i\n",
    "        index_to_word[i] = word\n",
    "        word_to_vec_map[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'the', (100,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['the'], index_to_word[0], word_to_vec_map['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Fill in the embedding matrix\n",
    "for word, index in word_to_index.items():\n",
    "    if word in word_to_vec_map:\n",
    "        embedding_matrix[index] = word_to_vec_map[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 2500\n",
    "\n",
    "def tokenize(sentences, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    tokenized_sentences = np.zeros((len(sentences), max_sentence_length))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = sentence.lower().split()\n",
    "        \n",
    "        for j, word in enumerate(words):    \n",
    "            if j == max_sentence_length:\n",
    "                break\n",
    "            \n",
    "            # Remove punctuation from the word\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "                        \n",
    "            if word in word_to_index:\n",
    "                tokenized_sentences[i, j] = word_to_index[word]\n",
    "            else:\n",
    "                tokenized_sentences[i, j] = 400000 # Vector for unknown words\n",
    "        \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenize(X_train_texts)\n",
    "X_test_sequences = tokenize(X_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "This model is a baseline for the other models. It is a simple neural network that consists of an embedding layer, a global average pooling layer, and a dense layer with sigmoid activation. **The embedding layer is trained from scratch.** Where the other models will use pre-trained embeddings, because training time would be too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences, y_train, epochs=20, batch_size=16, validation_data=(X_test_sequences, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- Small training set (1000 examples)\n",
    "- Training unitl reaches max test accuracy (20 to 60 epochs)\n",
    "- Batch size of 16\n",
    "- Adam optimizer\n",
    "\n",
    "| Model | Accuracy | Validation Accuracy |\n",
    "|-------|----------|---------------------|\n",
    "| Baseline | 1.0000 | 0.8260 |\n",
    "| LSTM 32 | 0.4970 | 0.5320 |\n",
    "| Bi-LSTM 8 | 0.9300 | 0.7460 |\n",
    "| 2 x Bi-LSTM 8 | 0.9350 | 0.7360 |\n",
    "| Bi-LSTM 16 | 0.9840 | 0.7860 |\n",
    "| Bi-LSTM 32 | 0.9990  | 0.7840 |\n",
    "| Bi-LSTM 128 | 0.9930 | 0.7320 |\n",
    "| 2 x Bi-LSTM 128 | 0.9990 | 0.7320 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Simple baseline model performs surprisingly well although it's overfitting\n",
    "- LSTM layers with > 32 units introduce overfitting **on a small dataset**. Training on a entire dataset should reduce overfitting. So we will try more units.\n",
    "- LSTM layers with < 16 units introduce biast - can't learn function well enough\n",
    "- Bidirectional LSTM layers give much better results\n",
    "- Adding second layer doesn't improve performance, only increases training time\n",
    "- Baseline model quickly reaches its best performance - it can't learn more complex function\n",
    "\n",
    "### Conclusions\n",
    "- Use single bidirectional LSTM layers with 16 > units\n",
    "- Train on entire dataset to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_SENTENCE_LENGTH, weights=[embedding_matrix], trainable=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences, y_train, epochs=10, batch_size=32, validation_data=(X_test_sequences, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 95.00%\n",
      "Train accuracy: 90.30%\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = model.evaluate(X_train_sequences, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test_sequences, y_test, verbose=0)\n",
    "\n",
    "print(f'Train accuracy: {train_acc*100:.2f}%')\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Final model got  **95.0%** accuracy on training set and **90.3%** accuracy on a test set.\n",
    "- Great! By training on entire dataset there is only little overfitting even with 64 units.\n",
    "- It's very hard to get over 90% test accuracy using this simple architecture. Units above 64 introduce overfitting. Adding more layers and layer normalization doesn't improve performance.\n",
    "- Models with 64 > units can get 99% training but only 90% test accuracy.\n",
    "- After 10 epochs there is only training accuracy improvement, test accuracy stays the same or even decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = [\n",
    "    \"The movie was great. I enjoyed every second of it although actors play wasn't perfect but plot made up for it. I would recommend it to others.\",\n",
    "    \"I loved this movie. It was so funny and entertaining.\",\n",
    "    \"This movie was terrible. The plot was boring and the acting was awful.\",\n",
    "    \"Funny movie but not an amazing one. Best to watch with your friends.\",\n",
    "    \"Not a good movie. I wasn't really satisfied with the actors play. I wouldn't recommend it to others.\",\n",
    "    \"A good movie. I was satisfied with the actors play. I would recommend it to others.\"\n",
    "]\n",
    "\n",
    "review_sequence = tokenize(review)\n",
    "\n",
    "prediction = model.predict(review_sequence).flatten() > 0.5\n",
    "\n",
    "# Map True to Positive and False to Negative\n",
    "np.where(prediction, \"Positive\", \"Negative\").tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
